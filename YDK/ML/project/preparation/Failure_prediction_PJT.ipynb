{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e03de07",
   "metadata": {},
   "source": [
    "# Failure Prediction PJT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab10584",
   "metadata": {},
   "source": [
    "## Machine Learning 프로젝트 수행을 위한 코드 구조화\n",
    "(분류, 회귀 Task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e5b8ed",
   "metadata": {},
   "source": [
    "### 템플릿 코드 생성 for ML Project\n",
    "1. 필요한 라이브러리와 데이터를 불러옵니다.\n",
    "\n",
    "\n",
    "2. EDA를 수행합니다. 이 때, EDA의 목적은 풀어야하는 문제를 위해서 수행됩니다.\n",
    "\n",
    "\n",
    "3. 전처리를 수행합니다. 이 때, 중요한 것은 feature engineering을 어떻게 하느냐 입니다.\n",
    "\n",
    "\n",
    "4. 데이터 분할을 진행합니다. 이 때, train data와 test tada 간의 분포 차이가 없는 지 확인합니다.\n",
    "\n",
    "\n",
    "5. 학습을 진행합니다. 어떤 모델을 사용하여 학습할지 정합니다. 성능이 잘 나오는 GBM을 추천합니다.\n",
    "\n",
    "\n",
    "6. Hyper-parameter tuning을 수행합니다. 원흐는 목표 성능이 나올 때 까지 진행합니다. \n",
    "   검증 단계를 통해, 지속적으로 overfitting이 되지 않게 주의하세요.\n",
    "\n",
    "\n",
    "7. 최종 테스트를 진행합니다. 데이터 대회 포맷에 맞는 submission 파일을 만들어서 성능을 확인해보세요.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a457f7d",
   "metadata": {},
   "source": [
    "## 1. 라이브러리, 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfd5f74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터분석 4종 세트\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 모델들, 성능 평가\n",
    "# 일반적으로 정형데이터로 머신러닝 분석을 진행할 때는 아래 2개 모델을 돌려봅니다. 특히 RandomForest가 테스트하기 좋습니다.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from lightgbm.sklearn import LGBMRegressor\n",
    "\n",
    "# 상관관계 분석, VIF : 다중공선성 제거\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# KFold(CV), partial : optuna를 사용하기 위함\n",
    "from sklearn.model_selection import KFold\n",
    "from functools import partial\n",
    "\n",
    "# hyper-parameter tuning을 위한 라이브러리, optuna\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f190c83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag setting\n",
    "data_reducing = False ## memory reducing technique\n",
    "feature_reducing = 'fi' ## 'pca' or 'fi'\n",
    "online = False ## use online training\n",
    "sampling = 'hybrid' ## 'under' or 'over' or 'hybrid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b8d77e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 970) (1000, 1157)\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 불러옵니다.\n",
    "\n",
    "# Related kaggle competitions\n",
    "# Name: Bosch Production Line Performance\n",
    "# Link: https://www.kaggle.com/competitions/bosch-production-line-performance\n",
    "\n",
    "# train_cat_part = pd.read_csv('../input/bosch-production-line-performance/train_categorical.csv.zip',\n",
    "#                             nrows=1000)\n",
    "train_num_part = pd.read_csv('./bosch-production-line-performance/train_numeric.csv.zip',\n",
    "                            nrows=1000)\n",
    "train_date_part = pd.read_csv('./bosch-production-line-performance/train_date.csv.zip',\n",
    "                             nrows=1000)\n",
    "\n",
    "#print(train_cat_part.shape, train_num_part.shape, train_date_part.shape)\n",
    "print(train_num_part.shape, train_date_part.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdaf537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical data는 EDA 결과 분석에 큰 도움이 없어, 사용하지 않겠습니다.\n",
    "\n",
    "# total = train_cat_part.count() * 0\n",
    "# N = 0\n",
    "\n",
    "# for chunk in pd.read_csv('../input/bosch-production-line-performance/train_categorical.csv.zip',\n",
    "#                         chunksize=100000):  # chunksize가 클수록, 속도는 빨라지고 메모리 사용량이 늘어납니다.\n",
    "    \n",
    "#     total = total + chunk.isnull().sum()  # 전체 데이터의 column별 결측치 개수\n",
    "#     N = N + len(chunk)  # 전체 데이터의 row 수\n",
    "    \n",
    "# display(total)\n",
    "# print(N)\n",
    "# usecols = total.index[(total / N) < 0.5] # train_categorical.csv에 있는 결측치가 50% 미만인 column들.\n",
    "# usecols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ad0eec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_cat = pd.read_csv('../input/bosch-production-line-performance/train_categorical.csv.zip',\n",
    "#                        usecols=usecols)\n",
    "# train_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b091e49d",
   "metadata": {},
   "source": [
    "train_date.csv에서는 파생 변수를 만들어서 사용합니다.\n",
    "\n",
    "\n",
    "1. 각 station별로 측정 시간이 모두 있거나, 없거나의 케이스만 존재한다.\n",
    "\n",
    "\n",
    "- 데이터 중에서 station별로 하나의 column만 뽑아서 체크해도 괜찮다.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "2. 각 station별로 시간이 모두 같지는 않다. 중간에 측정시간이 바뀌는 케이스도 있다.\n",
    "\n",
    "- 데이터 별로 min, max값을 계산하면 된다.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "- start_station : 해당 product가 처음으로 공정을 거친 station_name\n",
    "\n",
    "\n",
    "- end_station : 해당 product가 마지막으로 공정을 거친 station_name\n",
    "\n",
    "\n",
    "- time_diff : 해당 product가 처음 시작한 timestamp와 마지막으로 끝낸 timestamp의 차이 (공정에 걸린 시간)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c89856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1157 cols -> station 개수(S0 ~ S51)\n",
    "# 1. column들이 나와있는 series를 하나 만듭니다.\n",
    "train_date_part.count()\n",
    "# 2. 그 series의 index를 뽑아서 가운데 _S0_ 를 뽑습니다.\n",
    "train_date_part.count().reset_index()['index'].str.split('_', expand=True)[1]\n",
    "# 3. 각 station별로 하나씩만 남깁니다. + Id column 제거\n",
    "train_date_part.count().reset_index()['index'].str.split('_', expand=True)[1].drop_duplicates()[1:]\n",
    "# 4. 각 station별 index에 해당하는 column들을 뽑습니다.\n",
    "temp = train_date_part.count().reset_index()['index'].str.split('_', expand=True)[1].drop_duplicates()[1:].index\n",
    "datecols = train_date_part.columns[temp]\n",
    "datecols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4567807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_date_part 데이터를 가지고, 원하는 3개의 파생변수가 잘 생성되는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c870632",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_date_part[\"start_station\"] = -1  # 초기화(initialization)\n",
    "train_date_part[\"end_station\"] = -1\n",
    "\n",
    "for col in train_date_part[datecols]:\n",
    "    ## TODO : start_station과 end_station을 찾아주세요\n",
    "    station_name = col.split(\"_\")[1] # S0\n",
    "    station_num = int(col.split(\"_\")[1][1:]) # 0\n",
    "    notnulls = (~train_date_part[col].isnull()) # 해당 column에서 NaN 아닌 값들의 위치 (boolean mask)\n",
    "    \n",
    "    # NaN이라면 해당 station을 지나간 것. 아니라면 해당 station에서 측정이 된 것.\n",
    "    # start_station은 해당 column에서 \"처음으로\" 수치값이 등장해야한다.(NaN이 아님) 즉, start_station이 -1이며 notnull이어야 함.\n",
    "    # end_station은 해당 column에 수치값이 등장하면 매번 update한다. 즉, notnull이기만 하면 됨.\n",
    "    train_date_part.loc[(notnulls & (train_date_part.start_station == -1)), \"start_station\"] = station_num\n",
    "    train_date_part.loc[notnulls, \"end_station\"] = station_num\n",
    "\n",
    "start_time = train_date_part.drop(columns=[\"Id\", \"start_station\", \"end_station\"]).min(axis=1) # start_time\n",
    "end_time = train_date_part.drop(columns=[\"Id\", \"start_station\", \"end_station\"]).max(axis=1) # end_time\n",
    "time_diff = end_time - start_time  # 공정에 걸린 시간.\n",
    "train_date_part['time_diff'] = time_diff\n",
    "train_date = train_date_part[[\"Id\", \"start_station\", \"end_station\", \"time_diff\"]]\n",
    "train_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1437e387",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO: train_date.csv를 불러와서, start_station, end_station, time_diff 파생 변수를 생성한 뒤에,\n",
    "##        1183747 x 4 에 해당하는 train_date DataFrame을 만들어주세요.\n",
    "\n",
    "## [HINT]\n",
    "# 1. chunk (datecols)\n",
    "# 2. start, end station 만들었던거 그대로 사용\n",
    "# 3. min, max 계산해서 time_diff 구하기\n",
    "extract_cols = [\"Id\", \"start_station\", \"end_station\", \"time_diff\"]\n",
    "train_date = pd.DataFrame(columns=extract_cols)\n",
    "\n",
    "for chunk in pd.read_csv('../input/bosch-production-line-performance/train_date.csv.zip',\n",
    "                        usecols=[\"Id\"] + datecols.tolist(), chunksize=100000):\n",
    "\n",
    "    chunk[\"start_station\"] = -1  # 초기화(initialization)\n",
    "    chunk[\"end_station\"] = -1\n",
    "\n",
    "    for col in chunk[datecols]: # Id, start_station, end_station 제외한것과 같음.\n",
    "        ## TODO : start_station과 end_station을 찾아주세요\n",
    "        station_num = int(col.split(\"_\")[1][1:])\n",
    "        notnulls = (~chunk[col].isnull()) # 해당 column에서 NaN 아닌 값들의 위치 (boolean mask)\n",
    "\n",
    "        chunk.loc[(notnulls & (chunk.start_station == -1)), \"start_station\"] = station_num\n",
    "        chunk.loc[notnulls, \"end_station\"] = station_num\n",
    "        \n",
    "        \n",
    "    start_time = chunk[datecols].min(axis=1)\n",
    "    end_time = chunk[datecols].max(axis=1)\n",
    "    time_diff = end_time - start_time\n",
    "    chunk['time_diff'] = time_diff\n",
    "    chunk = chunk[extract_cols]\n",
    "    train_date = pd.concat([train_date, chunk])\n",
    "    \n",
    "train_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ef0680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_numeric.csv 데이터에서 중요한 feature를 뽑기 위해서, PCA(truncatedSVD) 또는 feature importance를 통해 뽑는 방법을 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379bb4a6",
   "metadata": {},
   "source": [
    "#### [WARNING] 이 데이터는 1000개의 데이터에 대해서만 계산한 값으로, 제대로 하려면 전체 데이터를 불러와서 계산해야 합니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59329483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_num.csv에서 결측치가 70% 이하인 데이터.\n",
    "nullcols = train_num_part.columns[train_num_part.isnull().mean() < 0.7]\n",
    "nullcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698a93a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if feature_reducing == 'pca':\n",
    "    from sklearn.decomposition import TruncatedSVD ## for sparse input\n",
    "\n",
    "    train_num = pd.read_csv('../input/bosch-production-line-performance/train_numeric.csv.zip',\n",
    "                           usecols=nullcols)\n",
    "\n",
    "    X = train_num.drop([\"Id\", \"Response\"], axis=1).fillna(0)  # 1183747 x 186 features\n",
    "    svd = TruncatedSVD(n_components=15, n_iter=30, random_state=42) ## more than 300 iterations!\n",
    "    X_svd = svd.fit_transform(X)\n",
    "    print(X_svd.shape) # 186 features -> 15 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cb12c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD로 만든 numpy array를 보기 좋게 DataFrame으로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad0a7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if feature_reducing == 'pca':\n",
    "    sv_cols = [f\"SV{i}\" for i in range(1, X_svd.shape[1]+1)]\n",
    "    df_svd = pd.DataFrame(data=X_svd, columns=sv_cols)\n",
    "    df_svd[\"Id\"] = train_num[\"Id\"] # Id column 추가\n",
    "    df_svd[\"Response\"] = train_num[\"Response\"] # target value 추가\n",
    "    display(df_svd)\n",
    "    \n",
    "    # release memory\n",
    "    del train_num\n",
    "    del X\n",
    "    del X_svd\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    # train_date에서 추출한 정보와, train_numeric에서 추출한 정보를 합쳐줍니다.\n",
    "    X_train = pd.merge(df_svd, train_date, on=\"Id\")\n",
    "    display(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090944db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestClassifier를 이용해서 feature importance가 높은 변수 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87538541",
   "metadata": {},
   "outputs": [],
   "source": [
    "if feature_reducing == 'fi' and False:  ## 현재 버젼에서는 training을 안시키려고 강제로 False로 만듬.\n",
    "    # Online learning (= batch training)\n",
    "    if online:\n",
    "        from tqdm.notebook import tqdm\n",
    "\n",
    "        clf = RandomForestClassifier(n_estimators=50, max_depth=31, max_features=0.7, n_jobs=-1,\n",
    "                                    random_state=42)#, warm_start=True)\n",
    "\n",
    "        cnt = 0\n",
    "        for chunk in pd.read_csv(\"../input/bosch-production-line-performance/train_numeric.csv.zip\",\n",
    "                                chunksize=100000):  # each iteration, training with 50000 rows\n",
    "            cnt += 1\n",
    "            X = chunk.drop([\"Id\", \"Response\"], axis=1).fillna(0)  # 10000 x 978 features\n",
    "            y = chunk.Response\n",
    "            clf.fit(X, y)\n",
    "            print(f\"in {cnt} iterations.\")\n",
    "     \n",
    "    # offline learning(=full-batch training)\n",
    "    else:\n",
    "        train_num = pd.read_csv('../input/bosch-production-line-performance/train_numeric.csv.zip',\n",
    "                               nrows=100000)\n",
    "\n",
    "        X = train_num.drop([\"Id\", \"Response\"], axis=1).fillna(0)  # 10000 x 978 features\n",
    "        y = train_num.Response\n",
    "\n",
    "        # max_features : sqrt(# of features) ~ 31\n",
    "        clf = RandomForestClassifier(n_estimators=50, max_depth=31, max_features=0.7, n_jobs=-1,\n",
    "                                    random_state=42)\n",
    "        clf.fit(X, y)\n",
    "        \n",
    "        feature_importances = clf.feature_importances_\n",
    "        fi_index = feature_importances.argsort()[::-1][:15] # feature_importance가 높은 순서대로의 index\n",
    "        usecols = train_num.columns[fi_index]\n",
    "        print(\"Features from RF: \", usecols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "798f559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using know Numeric Feature from below github link\n",
    "# https://github.com/aakashveera/bosch-production-line-performance/blob/master/Numeric%20Classifier.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38b2449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ee58b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1bb917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d8449c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fa6567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f2c61a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f904a42d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bfffe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22eed1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
